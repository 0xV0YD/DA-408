{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        },
        "id": "4ZgYm59VhfId",
        "outputId": "963e66df-fce3-45c8-df95-128a2e039cd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting MNIST training and quantization script using PyTorch...\n",
            "\n",
            "Training the model for 5 epochs...\n",
            "Epoch [1/5], Average Loss: 0.4762\n",
            "Epoch [2/5], Average Loss: 0.2517\n",
            "Epoch [3/5], Average Loss: 0.2044\n",
            "Epoch [4/5], Average Loss: 0.1735\n",
            "Epoch [5/5], Average Loss: 0.1508\n",
            "\n",
            "Test Accuracy: 95.41%\n",
            "\n",
            "Quantizing model parameters...\n",
            "\n",
            "Saving quantized data to files...\n",
            "\n",
            "Performing reference quantized computation...\n",
            "Layer 1 quantized outputs (first 10): tensor([     0, 119299, 107046, 181658, 211719, 117935, 176754,      0,      0,\n",
            "             0], dtype=torch.int32)\n",
            "Layer 2 quantized outputs: tensor([-22211535, -46329912,  -7189496,   7453657, -42984334, -25665783,\n",
            "        -69076711,  35531129, -15770282, -15694588], dtype=torch.int32)\n",
            "Predicted digit (quantized): 7\n",
            "Float model prediction: 7\n",
            "Actual label: 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Files saved in 'output/' directory:\n",
            "- weights1.mem, biases1.mem, weights2.mem, biases2.mem\n",
            "- input_image.mem\n",
            "- scales_pytorch.txt, hardware_scales.txt\n",
            "- comparison.png\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "print(\"Starting MNIST training and quantization script using PyTorch...\")\n",
        "\n",
        "# Step 1: Define the neural network architecture\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)  # Flatten\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Step 2: Load and preprocess MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Step 3: Instantiate model, loss, optimizer\n",
        "model = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Step 4: Train the model\n",
        "num_epochs = 5\n",
        "print(f\"\\nTraining the model for {num_epochs} epochs...\")\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct, total = 0, 0\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"\\nTest Accuracy: {(100 * correct / total):.2f}%\")\n",
        "\n",
        "# ---------------- QUANTIZATION HELPERS ----------------\n",
        "def quantize_to_int8(data, scale_factor=None):\n",
        "    \"\"\"Quantize tensor to int8 with symmetric scaling\"\"\"\n",
        "    if scale_factor is None:\n",
        "        max_abs_val = torch.max(torch.abs(data))\n",
        "        if max_abs_val == 0:\n",
        "            return torch.zeros_like(data, dtype=torch.int8), 1.0\n",
        "        scale_factor = max_abs_val / 127.0\n",
        "\n",
        "    quantized_data = torch.round(data / scale_factor)\n",
        "    quantized_data = torch.clamp(quantized_data, -128, 127)\n",
        "    return quantized_data.to(torch.int8), scale_factor\n",
        "\n",
        "def save_mem(filename, tensor):\n",
        "    \"\"\"Save tensor as .mem file (hex, signed 8-bit representation)\"\"\"\n",
        "    flat = tensor.flatten().cpu().numpy()\n",
        "    with open(filename, \"w\") as f:\n",
        "        for val in flat:\n",
        "            # Convert signed int8 to unsigned for hex representation\n",
        "            unsigned_val = int(val) & 0xFF\n",
        "            f.write(f\"{unsigned_val:02x}\\n\")\n",
        "\n",
        "def quantize_input_to_uint8(data):\n",
        "    \"\"\"Quantize input data (0-1 range) to uint8 (0-255)\"\"\"\n",
        "    # Scale from [0,1] to [0,255]\n",
        "    quantized = torch.round(data * 255.0)\n",
        "    quantized = torch.clamp(quantized, 0, 255)\n",
        "    return quantized.to(torch.uint8), 1.0/255.0\n",
        "\n",
        "# Step 6: Quantize weights and biases\n",
        "print(\"\\nQuantizing model parameters...\")\n",
        "weights_l1, scale_w1 = quantize_to_int8(model.fc1.weight.data)\n",
        "biases_l1, scale_b1 = quantize_to_int8(model.fc1.bias.data)\n",
        "weights_l2, scale_w2 = quantize_to_int8(model.fc2.weight.data)\n",
        "biases_l2, scale_b2 = quantize_to_int8(model.fc2.bias.data)\n",
        "\n",
        "# Step 7: Quantize input image\n",
        "sample_img, sample_label = test_dataset[0]\n",
        "sample_img_flat = sample_img.view(-1)\n",
        "\n",
        "# For input, we use uint8 quantization (0-255 range)\n",
        "quantized_input, scale_in = quantize_input_to_uint8(sample_img_flat)\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs('output', exist_ok=True)\n",
        "\n",
        "# Save quantized weights/biases (transposed for correct indexing)\n",
        "print(\"\\nSaving quantized data to files...\")\n",
        "np.savetxt('output/weights1_pytorch.txt', weights_l1.t().numpy(), fmt='%d', delimiter=',')\n",
        "np.savetxt('output/biases1_pytorch.txt', biases_l1.numpy(), fmt='%d')\n",
        "np.savetxt('output/weights2_pytorch.txt', weights_l2.t().numpy(), fmt='%d', delimiter=',')\n",
        "np.savetxt('output/biases2_pytorch.txt', biases_l2.numpy(), fmt='%d')\n",
        "\n",
        "# Save as .mem files\n",
        "save_mem(\"output/weights1.mem\", weights_l1.t())\n",
        "save_mem(\"output/biases1.mem\", biases_l1)\n",
        "save_mem(\"output/weights2.mem\", weights_l2.t())\n",
        "save_mem(\"output/biases2.mem\", biases_l2)\n",
        "\n",
        "# Save quantized input\n",
        "def save_mem_uint8(filename, tensor):\n",
        "    \"\"\"Save uint8 tensor as .mem file\"\"\"\n",
        "    flat = tensor.flatten().cpu().numpy()\n",
        "    with open(filename, \"w\") as f:\n",
        "        for val in flat:\n",
        "            f.write(f\"{int(val):02x}\\n\")\n",
        "\n",
        "save_mem_uint8(\"output/input_image.mem\", quantized_input)\n",
        "\n",
        "# Save all scales\n",
        "with open('output/scales_pytorch.txt', 'w') as f:\n",
        "    f.write(f'scale_in: {scale_in:.8f}\\n')\n",
        "    f.write(f'scale_w1: {scale_w1.item():.8f}\\n')\n",
        "    f.write(f'scale_b1: {scale_b1.item():.8f}\\n')\n",
        "    f.write(f'scale_w2: {scale_w2.item():.8f}\\n')\n",
        "    f.write(f'scale_b2: {scale_b2.item():.8f}\\n')\n",
        "\n",
        "# Calculate composite scales for hardware\n",
        "scale_l1_out = scale_in * scale_w1.item()\n",
        "scale_l2_out = scale_l1_out * scale_w2.item()\n",
        "\n",
        "with open('output/hardware_scales.txt', 'w') as f:\n",
        "    f.write(f'// Hardware scaling factors\\n')\n",
        "    f.write(f'// Layer 1 output scale: input_scale * weight1_scale\\n')\n",
        "    f.write(f'scale_l1_out: {scale_l1_out:.8f}\\n')\n",
        "    f.write(f'// Layer 2 output scale: layer1_out_scale * weight2_scale\\n')\n",
        "    f.write(f'scale_l2_out: {scale_l2_out:.8f}\\n')\n",
        "    f.write(f'// Scale factors as fixed-point (multiply by 2^16)\\n')\n",
        "    f.write(f'scale_l1_out_fp: {int(scale_l1_out * 65536)}\\n')\n",
        "    f.write(f'scale_l2_out_fp: {int(scale_l2_out * 65536)}\\n')\n",
        "\n",
        "# Step 8: Reference computation for verification\n",
        "print(\"\\nPerforming reference quantized computation...\")\n",
        "\n",
        "# Layer 1 computation (quantized)\n",
        "def quantized_linear(input_q, weight_q, bias_q, input_scale, weight_scale, bias_scale):\n",
        "    # Compute in int32\n",
        "    output = torch.zeros(weight_q.shape[0], dtype=torch.int32)\n",
        "    for j in range(weight_q.shape[0]):\n",
        "        acc = bias_q[j].int()\n",
        "        for i in range(weight_q.shape[1]):\n",
        "            acc += input_q[i].int() * weight_q[j, i].int()\n",
        "        output[j] = acc\n",
        "    return output\n",
        "\n",
        "l1_out_q = quantized_linear(quantized_input.int(), weights_l1, biases_l1,\n",
        "                           scale_in, scale_w1.item(), scale_b1.item())\n",
        "\n",
        "# Apply ReLU\n",
        "l1_out_q = torch.clamp(l1_out_q, 0, 2147483647)\n",
        "\n",
        "print(f\"Layer 1 quantized outputs (first 10): {l1_out_q[:10]}\")\n",
        "\n",
        "# Layer 2 computation\n",
        "l2_out_q = quantized_linear(l1_out_q, weights_l2, biases_l2,\n",
        "                           scale_l1_out, scale_w2.item(), scale_b2.item())\n",
        "\n",
        "print(f\"Layer 2 quantized outputs: {l2_out_q}\")\n",
        "predicted_digit = torch.argmax(l2_out_q)\n",
        "print(f\"Predicted digit (quantized): {predicted_digit.item()}\")\n",
        "\n",
        "# Step 9: Compare with float model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    float_output = model(sample_img.unsqueeze(0))\n",
        "    float_pred = torch.max(float_output.data, 1)[1]\n",
        "    print(f\"Float model prediction: {float_pred.item()}\")\n",
        "    print(f\"Actual label: {sample_label}\")\n",
        "\n",
        "# Step 10: Visualization\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(sample_img.squeeze(), cmap=\"gray\")\n",
        "plt.title(f\"Original Image\\nLabel: {sample_label}\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "quantized_img_display = quantized_input.float() / 255.0\n",
        "plt.imshow(quantized_img_display.view(28, 28), cmap=\"gray\")\n",
        "plt.title(f\"Quantized Image\\nPred: {predicted_digit.item()}\")\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig('output/comparison.png')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFiles saved in 'output/' directory:\")\n",
        "print(\"- weights1.mem, biases1.mem, weights2.mem, biases2.mem\")\n",
        "print(\"- input_image.mem\")\n",
        "print(\"- scales_pytorch.txt, hardware_scales.txt\")\n",
        "print(\"- comparison.png\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "OUTPUT_FILE = \"output/input.mem\"   # File to save quantized image\n",
        "IMAGE_INDEX = 6                    # New image index (10th image)\n",
        "os.makedirs(\"output\", exist_ok=True)\n",
        "\n",
        "# ----------------- LOAD MNIST -----------------\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Get a single image and label\n",
        "sample_img, sample_label = test_dataset[IMAGE_INDEX]\n",
        "sample_img_flat = sample_img.view(-1)  # Flatten 28x28 to 784\n",
        "\n",
        "# ----------------- QUANTIZE TO UINT8 -----------------\n",
        "quantized_input = torch.clamp(torch.round(sample_img_flat * 255.0), 0, 255).to(torch.uint8)\n",
        "\n",
        "# ----------------- SAVE AS .MEM -----------------\n",
        "with open(OUTPUT_FILE, \"w\") as f:\n",
        "    for val in quantized_input:\n",
        "        hex_val = f\"{int(val):02x}\"\n",
        "        f.write(f\"{hex_val}\\n\")\n",
        "        print(hex_val, end=' ')  # Print hex value to console\n",
        "print(f\"\\n\\nQuantized image saved to {OUTPUT_FILE}\")\n",
        "print(f\"Actual label: {sample_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ft3SOq0Xx1gA",
        "outputId": "4c3b7f74-0926-4047-c4a1-5833282c8cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 16 c0 86 20 00 00 00 00 00 00 00 00 0f 4d 05 00 00 00 00 00 00 00 00 00 00 00 00 11 eb fa a9 00 00 00 00 00 00 00 00 0f dc f1 25 00 00 00 00 00 00 00 00 00 00 00 14 bd fd 93 00 00 00 00 00 00 00 00 00 8b fd 64 00 00 00 00 00 00 00 00 00 00 00 00 46 fd fd 15 00 00 00 00 00 00 00 00 2b fe ad 0d 00 00 00 00 00 00 00 00 00 00 00 16 99 fd 60 00 00 00 00 00 00 00 00 2b e7 fe 5c 00 00 00 00 00 00 00 00 00 00 00 00 a3 ff cc 0b 00 00 00 00 00 00 00 00 68 fe 9e 00 00 00 00 00 00 00 00 00 00 00 00 00 a2 fd b2 05 00 00 00 00 00 00 09 83 ed fd 00 00 00 00 00 00 00 00 00 00 00 00 00 00 a2 fd fd bf af 46 46 46 46 85 c5 fd fd a9 00 00 00 00 00 00 00 00 00 00 00 00 00 00 33 e4 fd fd fe fd fd fd fd fe fd fd db 23 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 11 41 89 fe e8 89 89 89 2c fd fd a1 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 22 fe ce 15 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 a0 fd 45 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 55 fe f1 32 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 9e fe a5 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 e7 f4 32 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 68 fe e8 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 d0 fd 9d 00 0d 1e 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 d0 fd 9a 5b cc a1 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 d0 fd fe fd 9a 1d 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 3d be 80 17 06 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 \n",
            "\n",
            "Quantized image saved to output/input.mem\n",
            "Actual label: 4\n"
          ]
        }
      ]
    }
  ]
}