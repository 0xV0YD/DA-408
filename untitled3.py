# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QQVD51LABlTnjpJArEmJ8LkhXFzRbDco
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt
import os

print("Starting MNIST training and quantization script using PyTorch...")

# Step 1: Define the neural network architecture
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(784, 32)
        self.fc2 = nn.Linear(32, 10)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.view(-1, 784)  # Flatten
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Step 2: Load and preprocess MNIST dataset
transform = transforms.Compose([transforms.ToTensor()])
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)

# Step 3: Instantiate model, loss, optimizer
model = SimpleNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Step 4: Train the model
num_epochs = 5
print(f"\nTraining the model for {num_epochs} epochs...")
for epoch in range(num_epochs):
    running_loss = 0.0
    for batch_idx, (images, labels) in enumerate(train_loader):
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    avg_loss = running_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}")

# Step 5: Evaluate the model
model.eval()
with torch.no_grad():
    correct, total = 0, 0
    for images, labels in test_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f"\nTest Accuracy: {(100 * correct / total):.2f}%")

# ---------------- QUANTIZATION HELPERS ----------------
def quantize_to_int8(data, scale_factor=None):
    """Quantize tensor to int8 with symmetric scaling"""
    if scale_factor is None:
        max_abs_val = torch.max(torch.abs(data))
        if max_abs_val == 0:
            return torch.zeros_like(data, dtype=torch.int8), 1.0
        scale_factor = max_abs_val / 127.0

    quantized_data = torch.round(data / scale_factor)
    quantized_data = torch.clamp(quantized_data, -128, 127)
    return quantized_data.to(torch.int8), scale_factor

def save_mem(filename, tensor):
    """Save tensor as .mem file (hex, signed 8-bit representation)"""
    flat = tensor.flatten().cpu().numpy()
    with open(filename, "w") as f:
        for val in flat:
            # Convert signed int8 to unsigned for hex representation
            unsigned_val = int(val) & 0xFF
            f.write(f"{unsigned_val:02x}\n")

def quantize_input_to_uint8(data):
    """Quantize input data (0-1 range) to uint8 (0-255)"""
    # Scale from [0,1] to [0,255]
    quantized = torch.round(data * 255.0)
    quantized = torch.clamp(quantized, 0, 255)
    return quantized.to(torch.uint8), 1.0/255.0

# Step 6: Quantize weights and biases
print("\nQuantizing model parameters...")
weights_l1, scale_w1 = quantize_to_int8(model.fc1.weight.data)
biases_l1, scale_b1 = quantize_to_int8(model.fc1.bias.data)
weights_l2, scale_w2 = quantize_to_int8(model.fc2.weight.data)
biases_l2, scale_b2 = quantize_to_int8(model.fc2.bias.data)

# Step 7: Quantize input image
sample_img, sample_label = test_dataset[0]
sample_img_flat = sample_img.view(-1)

# For input, we use uint8 quantization (0-255 range)
quantized_input, scale_in = quantize_input_to_uint8(sample_img_flat)

# Create output directory if it doesn't exist
os.makedirs('output', exist_ok=True)

# Save quantized weights/biases (transposed for correct indexing)
print("\nSaving quantized data to files...")
np.savetxt('output/weights1_pytorch.txt', weights_l1.t().numpy(), fmt='%d', delimiter=',')
np.savetxt('output/biases1_pytorch.txt', biases_l1.numpy(), fmt='%d')
np.savetxt('output/weights2_pytorch.txt', weights_l2.t().numpy(), fmt='%d', delimiter=',')
np.savetxt('output/biases2_pytorch.txt', biases_l2.numpy(), fmt='%d')

# Save as .mem files
save_mem("output/weights1.mem", weights_l1.t())
save_mem("output/biases1.mem", biases_l1)
save_mem("output/weights2.mem", weights_l2.t())
save_mem("output/biases2.mem", biases_l2)

# Save quantized input
def save_mem_uint8(filename, tensor):
    """Save uint8 tensor as .mem file"""
    flat = tensor.flatten().cpu().numpy()
    with open(filename, "w") as f:
        for val in flat:
            f.write(f"{int(val):02x}\n")

save_mem_uint8("output/input_image.mem", quantized_input)

# Save all scales
with open('output/scales_pytorch.txt', 'w') as f:
    f.write(f'scale_in: {scale_in:.8f}\n')
    f.write(f'scale_w1: {scale_w1.item():.8f}\n')
    f.write(f'scale_b1: {scale_b1.item():.8f}\n')
    f.write(f'scale_w2: {scale_w2.item():.8f}\n')
    f.write(f'scale_b2: {scale_b2.item():.8f}\n')

# Calculate composite scales for hardware
scale_l1_out = scale_in * scale_w1.item()
scale_l2_out = scale_l1_out * scale_w2.item()

with open('output/hardware_scales.txt', 'w') as f:
    f.write(f'// Hardware scaling factors\n')
    f.write(f'// Layer 1 output scale: input_scale * weight1_scale\n')
    f.write(f'scale_l1_out: {scale_l1_out:.8f}\n')
    f.write(f'// Layer 2 output scale: layer1_out_scale * weight2_scale\n')
    f.write(f'scale_l2_out: {scale_l2_out:.8f}\n')
    f.write(f'// Scale factors as fixed-point (multiply by 2^16)\n')
    f.write(f'scale_l1_out_fp: {int(scale_l1_out * 65536)}\n')
    f.write(f'scale_l2_out_fp: {int(scale_l2_out * 65536)}\n')

# Step 8: Reference computation for verification
print("\nPerforming reference quantized computation...")

# Layer 1 computation (quantized)
def quantized_linear(input_q, weight_q, bias_q, input_scale, weight_scale, bias_scale):
    # Compute in int32
    output = torch.zeros(weight_q.shape[0], dtype=torch.int32)
    for j in range(weight_q.shape[0]):
        acc = bias_q[j].int()
        for i in range(weight_q.shape[1]):
            acc += input_q[i].int() * weight_q[j, i].int()
        output[j] = acc
    return output

l1_out_q = quantized_linear(quantized_input.int(), weights_l1, biases_l1,
                           scale_in, scale_w1.item(), scale_b1.item())

# Apply ReLU
l1_out_q = torch.clamp(l1_out_q, 0, 2147483647)

print(f"Layer 1 quantized outputs (first 10): {l1_out_q[:10]}")

# Layer 2 computation
l2_out_q = quantized_linear(l1_out_q, weights_l2, biases_l2,
                           scale_l1_out, scale_w2.item(), scale_b2.item())

print(f"Layer 2 quantized outputs: {l2_out_q}")
predicted_digit = torch.argmax(l2_out_q)
print(f"Predicted digit (quantized): {predicted_digit.item()}")

# Step 9: Compare with float model
model.eval()
with torch.no_grad():
    float_output = model(sample_img.unsqueeze(0))
    float_pred = torch.max(float_output.data, 1)[1]
    print(f"Float model prediction: {float_pred.item()}")
    print(f"Actual label: {sample_label}")

# Step 10: Visualization
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.imshow(sample_img.squeeze(), cmap="gray")
plt.title(f"Original Image\nLabel: {sample_label}")
plt.axis('off')

plt.subplot(1, 2, 2)
quantized_img_display = quantized_input.float() / 255.0
plt.imshow(quantized_img_display.view(28, 28), cmap="gray")
plt.title(f"Quantized Image\nPred: {predicted_digit.item()}")
plt.axis('off')
plt.tight_layout()
plt.savefig('output/comparison.png')
plt.show()

print("\nFiles saved in 'output/' directory:")
print("- weights1.mem, biases1.mem, weights2.mem, biases2.mem")
print("- input_image.mem")
print("- scales_pytorch.txt, hardware_scales.txt")
print("- comparison.png")

import torch
from torchvision import datasets, transforms
import os

# ----------------- CONFIG -----------------
OUTPUT_FILE = "output/input.mem"   # File to save quantized image
IMAGE_INDEX = 6                    # New image index (10th image)
os.makedirs("output", exist_ok=True)

# ----------------- LOAD MNIST -----------------
transform = transforms.Compose([transforms.ToTensor()])
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

# Get a single image and label
sample_img, sample_label = test_dataset[IMAGE_INDEX]
sample_img_flat = sample_img.view(-1)  # Flatten 28x28 to 784

# ----------------- QUANTIZE TO UINT8 -----------------
quantized_input = torch.clamp(torch.round(sample_img_flat * 255.0), 0, 255).to(torch.uint8)

# ----------------- SAVE AS .MEM -----------------
with open(OUTPUT_FILE, "w") as f:
    for val in quantized_input:
        hex_val = f"{int(val):02x}"
        f.write(f"{hex_val}\n")
        print(hex_val, end=' ')  # Print hex value to console
print(f"\n\nQuantized image saved to {OUTPUT_FILE}")
print(f"Actual label: {sample_label}")